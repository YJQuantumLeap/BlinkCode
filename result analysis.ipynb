{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result analysis - the score of each model in each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================qwen-vl-max====================\n",
      "====================claude-3-5-sonnet-20240620====================\n",
      "**********second_refine**********\n",
      "HumanEval-V correctness: 1.0\n",
      "MBPP-V correctness: 0.5\n",
      "GSM8K-V correctness: 0.5\n",
      "MATH-V correctness: 0.5\n",
      "VP correctness: 0.0\n",
      "Webpage score: 4.0\n",
      "Matplotlib score: 4.0\n",
      "SVG score: 2.0\n",
      "TikZ score: 3.5\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "import os\n",
    "import os\n",
    "import json\n",
    "\n",
    "def stream_jsonl(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            yield json.loads(line)\n",
    "\n",
    "models_names = ['qwen-vl-max', 'claude-3-5-sonnet-20240620']\n",
    "paths = ['not_refine/ouput_results.jsonl', 'refine/output_1_results.jsonl', 'refine/output_2_results.jsonl']\n",
    "tasks = ['Webpage', 'Matplotlib', 'HumanEval-V', 'MBPP-V', 'GSM8K-V', 'MATH-V', 'SVG', 'TikZ', 'VP']\n",
    "#\n",
    "data = defaultdict(lambda: defaultdict(dict))\n",
    "def recursive_defaultdict():\n",
    "    return defaultdict(recursive_defaultdict)\n",
    "error_data = recursive_defaultdict()\n",
    "for model_name in models_names:\n",
    "    print(f'===================={model_name}====================')\n",
    "    for path in paths:\n",
    "        file_path = f\"output/{model_name}/{path}\"\n",
    "        result_analysis = defaultdict(lambda: defaultdict(int))\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "        if \"not_refine\"  in file_path:\n",
    "            file_name = \"not_refine\"\n",
    "        elif \"output_1\" in file_path:\n",
    "            file_name = \"first_refine\"\n",
    "        else:\n",
    "            file_name = \"second_refine\"\n",
    "        print(\"*\" * 10 + f'{file_name}' + \"*\" * 10)\n",
    "        total = defaultdict(list)\n",
    "        correct = defaultdict(list)\n",
    "        score = defaultdict(list)\n",
    "        results = defaultdict(list)\n",
    "        results = stream_jsonl(file_path)\n",
    "        for result in results:\n",
    "            task_type = result['type']\n",
    "            if task_type in ['HumanEval-V', 'MBPP-V', 'GSM8K-V', 'MATH-V', 'VP']:\n",
    "                correct[task_type].append(result['passed'])\n",
    "            else:\n",
    "                score[task_type].append(result[\"score\"])\n",
    "        evaluate_result = defaultdict(list)\n",
    "        keys = list(correct.keys()) + list(score.keys())\n",
    "        for task_type in keys:\n",
    "            if task_type in ['HumanEval-V', 'MBPP-V', 'GSM8K-V', 'MATH-V', 'VP']:\n",
    "                evaluate_result[task_type] = sum(correct[task_type]) / len(correct[task_type])\n",
    "                print(f'{task_type} correctness: {evaluate_result[task_type]}')\n",
    "            else:\n",
    "                evaluate_result[task_type] = sum(score[task_type]) / len(score[task_type])\n",
    "                print(f'{task_type} score: {evaluate_result[task_type]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files with Chinese characters found:\n",
      "/home/huangyajun/data/dataset_MLLM_code_ability/README.md\n",
      "/home/huangyajun/data/dataset_MLLM_code_ability/output/qwen-vl-max/refine/output_1.jsonl\n",
      "/home/huangyajun/data/dataset_MLLM_code_ability/output/qwen-vl-max/refine/output_2.jsonl\n",
      "/home/huangyajun/data/dataset_MLLM_code_ability/output/qwen-vl-max/refine_/output_1.jsonl\n",
      "/home/huangyajun/data/dataset_MLLM_code_ability/output/qwen-vl-max/refine_/output_2.jsonl\n",
      "/home/huangyajun/data/dataset_MLLM_code_ability/output/qwen-vl-max/not_refine/output.jsonl\n",
      "/home/huangyajun/data/dataset_MLLM_code_ability/output/claude-3-5-sonnet-20240620/refine/output_1.jsonl\n",
      "/home/huangyajun/data/dataset_MLLM_code_ability/output/claude-3-5-sonnet-20240620/refine/output_2.jsonl\n",
      "/home/huangyajun/data/dataset_MLLM_code_ability/output/claude-3-5-sonnet-20240620/not_refine/output.jsonl\n",
      "/home/huangyajun/data/dataset_MLLM_code_ability/logs/refine_ouput/claude-3-5-sonnet-20240620.txt\n",
      "/home/huangyajun/data/dataset_MLLM_code_ability/logs/refine_ouput/qwen-vl-max.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def contains_chinese_characters(string):\n",
    "\n",
    "    return re.search(r'[\\u4e00-\\u9fff]', string) is not None\n",
    "\n",
    "def check_file_for_chinese_characters(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                if contains_chinese_characters(line):\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        # print(f\"Error reading file {file_path}: {e}\")\\\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def recursive_search(directory):\n",
    "    files_with_chinese = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            if \"tmp_file\" in full_path:\n",
    "                continue\n",
    "            if os.path.isfile(full_path) and check_file_for_chinese_characters(full_path):\n",
    "                files_with_chinese.append(full_path)\n",
    "    return files_with_chinese\n",
    "\n",
    "\n",
    "directory_to_search = '/home/huangyajun/data/dataset_MLLM_code_ability'\n",
    "\n",
    "\n",
    "files_with_chinese = recursive_search(directory_to_search)\n",
    "\n",
    "if files_with_chinese:\n",
    "    print(\"Files with Chinese characters found:\")\n",
    "    for file in files_with_chinese:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"No files with Chinese characters found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json \n",
    "import shutil\n",
    "from utils import get_code\n",
    "def read_map_jsonl(file_path):\n",
    "    map_data = defaultdict(list)\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            map_data[data['type']].append(data)\n",
    "    return map_data\n",
    "\n",
    "def add_write_jsonl(file_path, data):\n",
    "    with open(file_path, 'a') as file:\n",
    "        json.dump(data, file)\n",
    "        file.write('\\n')\n",
    "# function_name, id, evaluate_funciton\n",
    "data = read_map_jsonl('data/full_data.jsonl')\n",
    "tasks = ['Webpage', 'Matplotlib', 'HumanEval-V', 'MBPP-V', 'GSM8K-V', 'MATH-V', 'SVG', 'TikZ', 'VP']\n",
    "# base_prompt = {}\n",
    "# for type, d in data.items():\n",
    "#     base_prompt[type] = d[0]['prompt']\n",
    "with open('prompt/base_prompt.json', 'w') as file:\n",
    "    json.dump(base_prompt, file, indent=4)\n",
    "for type, d_list in data.items():\n",
    "    start_id = 0\n",
    "    image_path = f'data/{type}/images'\n",
    "    file_path = f'data/dataset.jsonl'\n",
    "    # image_path = f'data/full_data/images'\n",
    "    # file_path = f'data/full_data/dataset.jsonl'\n",
    "    os.makedirs(image_path, exist_ok=True)\n",
    "    if type in ['HumanEval-V', 'GSM8K-V', 'MATH-V', 'MBPP-V']:\n",
    "        for d in d_list:\n",
    "            dict_ = {}\n",
    "            dict_['id'] = start_id\n",
    "            s_id = str(start_id)\n",
    "            if len(s_id) == 1:\n",
    "                s_id = '00' + s_id\n",
    "            elif len(s_id) == 2:\n",
    "                s_id = '0' + s_id\n",
    "            dict_['path'] = f\"{type}/images/{s_id}.png\"\n",
    "            dict_['function_name'] = d['entry_point'].strip()\n",
    "            if type == 'MBPP-V':\n",
    "                dict_['function_name'] = get_code(d['prompt'], '```python\\ndef ', \":\\n```\")\n",
    "                # if dict_['function_name'] not in dict_['function_definition']:\n",
    "                #     raise ValueError(\"Function definition not found\")\n",
    "                # print(dict_['function_name'].split('(')[0])\n",
    "                # if dict_['function_name'].split('(')[0] != d['entry_point'].strip():\n",
    "                #     print(f'{dict_[\"function_name\"]} {d[\"entry_point\"]}')\n",
    "                #     print(d['task_id'])\n",
    "            dict_['evaluation_function'] = d['test']\n",
    "            dict_['type'] = type\n",
    "            now_image_path = d['task_id'] + \".png\"\n",
    "            target_image_path  = f'{image_path}/{s_id}.png'\n",
    "            shutil.copy(now_image_path, target_image_path)\n",
    "            add_write_jsonl(file_path, dict_)\n",
    "            start_id += 1\n",
    "    elif type == \"VP\":\n",
    "        for d in d_list:\n",
    "            dict_ = {}\n",
    "            dict_['id'] = start_id\n",
    "            s_id = str(start_id)\n",
    "            if len(s_id) == 1:\n",
    "                s_id = '00' + s_id\n",
    "            elif len(s_id) == 2:\n",
    "                s_id = '0' + s_id\n",
    "            dict_['path'] = f\"{type}/images/{s_id}.png\"\n",
    "            dict_['ocr_result'] = get_code(d['prompt'], \"####################OCR result####################\\n\", \"\\n####################OCR result####################\")\n",
    "            dict_['query'] = get_code(d['prompt'], \"######OCR result####################\\n\\nQuery: \", \"\\n\\nIf you think you can answer the question\")\n",
    "            dict_['reference_answer'] = d['test']\n",
    "            dict_['type'] = type\n",
    "            now_image_path = d['task_id'] + \".png\"\n",
    "            target_image_path  = f'{image_path}/{s_id}.png'\n",
    "            shutil.copy(now_image_path, target_image_path)\n",
    "            add_write_jsonl(file_path, dict_)\n",
    "            start_id += 1\n",
    "    else:\n",
    "        for d in d_list:\n",
    "            dict_ = {}\n",
    "            dict_['id'] = start_id\n",
    "            s_id = str(start_id)\n",
    "            if len(s_id) == 1:\n",
    "                s_id = '00' + s_id\n",
    "            elif len(s_id) == 2:\n",
    "                s_id = '0' + s_id\n",
    "            dict_['path'] = f\"{type}/images/{s_id}.png\"\n",
    "            # if type == 'SVG':\n",
    "            #     dict_['original_svg_code'] = d['test']\n",
    "            dict_['type'] = type\n",
    "            now_image_path = d['task_id'] + \".png\"\n",
    "            target_image_path  = f'{image_path}/{s_id}.png'\n",
    "            shutil.copy(now_image_path, target_image_path)\n",
    "            add_write_jsonl(file_path, dict_)\n",
    "            start_id += 1\n",
    "        # for d in d_list:\n",
    "        #     print(f'{d[\"function_name\"]} {d[\"id\"]} {d[\"evaluate_function\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
